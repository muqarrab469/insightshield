{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfn4Tg3YXFv0WPFiaFhpcA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muqarrab469/insightshield/blob/main/BBCScraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BBC News Scraper**"
      ],
      "metadata": {
        "id": "w4nMuqWKo2VT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc0rcb3LoyrM",
        "outputId": "84ecfe37-a1b0-40e5-9ce5-843c3e9b48e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Mounted at /content/drive\n",
            "Scraping article links...\n",
            "Scraped 10 article links.\n",
            "Scraping article data...\n",
            "Scraping article 1 data from: https://www.bbc.com/news/science-environment-58073295\n",
            "Scraping article 2 data from: https://www.bbc.com/news/articles/cg33rlzxgnxo\n",
            "Scraping article 3 data from: https://www.bbc.com/news/articles/cqlle3k92zqo\n",
            "Scraping article 4 data from: https://www.bbc.com/travel/article/20240605-f1-driver-lance-strolls-guide-to-a-weekend-in-his-hometown-montreal\n",
            "Scraping article 5 data from: https://www.bbc.com/travel/article/20240603-the-pacific-crest-trail-the-us-west-coasts-greatest-footpath\n",
            "Scraping article 6 data from: https://www.bbc.com/news/articles/crggqrzyjjpo\n",
            "Scraping article 7 data from: https://www.bbc.com/news/articles/c800xr94x5lo\n",
            "Error occurred while scraping article data from https://www.bbc.com/news/articles/c800xr94x5lo: 'NoneType' object has no attribute 'find_all'\n",
            "Scraping article 8 data from: https://www.bbc.com/news/articles/c9001dkzxeno\n",
            "Scraping article 9 data from: https://www.bbc.co.uk/sounds\n",
            "Error occurred while scraping article data from https://www.bbc.co.uk/sounds: 'NoneType' object has no attribute 'find_all'\n",
            "Scraping article 10 data from: https://www.bbc.com/news/articles/cjqq7zw5g27o\n",
            "Saving scraped data to CSV file...\n",
            "Scraping complete. Data saved to '/content/drive/My Drive/FYP/bbc_articles.csv'\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "output_file = \"/content/drive/My Drive/FYP/bbc_articles.csv\"\n",
        "IMAGE_DIR = '/content/drive/My Drive/FYP/bbc_images'\n",
        "\n",
        "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
        "BASE_URL = 'https://www.bbc.com'\n",
        "\n",
        "#Articles Link Scraper Function:\n",
        "def scrape_article_links(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        article_links = set()\n",
        "\n",
        "        for link in soup.find_all('a', class_='sc-2e6baa30-0 gILusN'):\n",
        "            href = link.get('href')\n",
        "            if href:\n",
        "                full_url = BASE_URL + href if href.startswith('/') else href\n",
        "                article_links.add(full_url)\n",
        "\n",
        "        return list(article_links)[:10]\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred while scraping article links: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def download_image(image_url, article_id):\n",
        "    try:\n",
        "        response = requests.get(image_url, stream=True)\n",
        "        if response.status_code == 200:\n",
        "            image_path = os.path.join(IMAGE_DIR, f\"news_{article_id}.jpg\")\n",
        "            with open(image_path, 'wb') as file:\n",
        "                for chunk in response.iter_content(1024):\n",
        "                    file.write(chunk)\n",
        "            return image_path\n",
        "        else:\n",
        "            print(f\"Failed to download image from {image_url}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred while downloading image from {image_url}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def convert_time_format(time_string):\n",
        "    if 'hour' in time_string:\n",
        "        hours_ago = int(time_string.split()[0])\n",
        "        published_time = datetime.now() - timedelta(hours=hours_ago)\n",
        "    else:\n",
        "        published_time = datetime.now()  # Default to current time if unable to parse\n",
        "    return published_time.strftime('%B %d, %Y / %I:%M %p %Z')\n",
        "\n",
        "#Scraper to Scrap from Scraped Links:\n",
        "def scrape_article_data(article_link, article_id):\n",
        "    try:\n",
        "        response = requests.get(article_link)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        #Title\n",
        "        title_tag = soup.find('h1', class_='sc-518485e5-0 bWszMR')\n",
        "        title = title_tag.text.strip() if title_tag else None\n",
        "\n",
        "        #Body\n",
        "        body = ''\n",
        "        text_blocks = soup.find_all('div', class_='sc-43e6b7ba-0 bWSguZ')\n",
        "        for block in text_blocks:\n",
        "            paragraphs = block.find_all('p')\n",
        "            for paragraph in paragraphs:\n",
        "                body += paragraph.text.strip() + ' '\n",
        "        body = body.strip()\n",
        "\n",
        "        #Category\n",
        "        category = 'World'\n",
        "\n",
        "        #Image URL\n",
        "        img_tag = soup.find('img', class_='sc-fd6cb93-0 hvRJnO')\n",
        "        article_img = img_tag.get('src') if img_tag else None\n",
        "        if article_img:\n",
        "            article_img = download_image(article_img, article_id)\n",
        "\n",
        "        #Date and time\n",
        "        date_tag = soup.find('time', class_='sc-36c5adb2-9 bhJTar')\n",
        "        article_time_string = date_tag.text.strip() if date_tag else 'Unknown'\n",
        "        article_date_time = convert_time_format(article_time_string)\n",
        "\n",
        "        #Author\n",
        "        author_tags = soup.find('div', class_='sc-36c5adb2-3 eDSSZR').find_all(['span', 'a'], class_=['sc-36c5adb2-5 fiZctN', 'sc-36c5adb2-6 hzkUOS'])\n",
        "        author = ', '.join(tag.text.strip() for tag in author_tags)\n",
        "        article_author = author.strip()\n",
        "\n",
        "        #Website name\n",
        "        article_website_name = 'BBC'\n",
        "\n",
        "        return {\n",
        "            'id': article_id,\n",
        "            'news_title': title,\n",
        "            'news_text': body,\n",
        "            'news_category': category,\n",
        "            'news_img': article_img,\n",
        "            'news_date_time': article_date_time,\n",
        "            'news_author': article_author,\n",
        "            'news_website_name': article_website_name,\n",
        "            'news_link': article_link\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred while scraping article data from {article_link}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    base_url = 'https://www.bbc.com'\n",
        "\n",
        "    print(\"Scraping article links...\")\n",
        "    article_links = scrape_article_links(base_url)\n",
        "    print(f\"Scraped {len(article_links)} article links.\")\n",
        "\n",
        "    articles_data = []\n",
        "\n",
        "    print(\"Scraping article data...\")\n",
        "    for idx, link in enumerate(article_links, start=1):\n",
        "        print(f\"Scraping article {idx} data from: {link}\")\n",
        "        article_data = scrape_article_data(link, idx)\n",
        "        if article_data:\n",
        "            articles_data.append(article_data)\n",
        "\n",
        "    print(\"Saving scraped data to CSV file...\")\n",
        "    with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        fieldnames = [\"id\", \"news_title\", \"news_text\", \"news_category\", \"news_img\", \"news_date_time\", \"news_author\", \"news_website_name\", \"news_link\"]\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for article in articles_data:\n",
        "            writer.writerow(article)\n",
        "\n",
        "    print(f\"Scraping complete. Data saved to '{output_file}'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}