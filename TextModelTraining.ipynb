{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8y251DXntdPL1QcLR+Y5o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muqarrab469/insightshield/blob/main/TextModelTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Trainning and Testing**"
      ],
      "metadata": {
        "id": "scsXPYcTZU70"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Drive Mounting:"
      ],
      "metadata": {
        "id": "FmvnKunYbPGY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LEIb5oRZP7h",
        "outputId": "60d5e1a0-3827-40bc-a483-95e36838d169"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Sets Loading:"
      ],
      "metadata": {
        "id": "HP8LVAlpbTus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define file paths\n",
        "real_news_path = '/content/drive/My Drive/FYP/FYPDataSet/TrueNewsData.csv'\n",
        "fake_news_path = '/content/drive/My Drive/FYP/FYPDataSet/FakeNewsData.csv'\n",
        "\n",
        "real_df = pd.read_csv(real_news_path, encoding='ISO-8859-1')\n",
        "fake_df = pd.read_csv(fake_news_path, encoding='ISO-8859-1')\n",
        "\n",
        "#Labels\n",
        "#real_df['label'] = 'Real'\n",
        "#fake_df['label'] = 'Fake'\n",
        "\n",
        "#Combinig Datasets of Real and Fake News\n",
        "df = pd.concat([real_df, fake_df], ignore_index=True)\n"
      ],
      "metadata": {
        "id": "nxJM946fbaG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing:"
      ],
      "metadata": {
        "id": "qdgqQaOScYah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip instal pandas\n",
        "!pip install nltk\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import unicodedata\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "#Function for Preprocessing:\n",
        "def preprocess_text(text):\n",
        "\n",
        "    #To remove HTML tags\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "    #To lowercase data\n",
        "    text = text.lower()\n",
        "\n",
        "    #To Rremove punctuation marks\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    #To remove numbers using regular expression\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    #To remove non-ASCII characters\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n",
        "\n",
        "    #Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    #To remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "\n",
        "    #To join tokens\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "\n",
        "df['processed_title'] = df['news_title'].apply(preprocess_text)\n",
        "df['processed_text'] = df['news_text'].apply(preprocess_text)\n",
        "\n",
        "# Display the DataFrame with processed text\n",
        "print(df[['news_title', 'processed_title', 'news_text', 'processed_text']])\n"
      ],
      "metadata": {
        "id": "_nbQJJ6cexss",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "623d0cc9-a652-4ff0-ae32-082aa6100561"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "ERROR: unknown command \"instal\" - maybe you meant \"install\"\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "<ipython-input-38-ea37174c58eb>:22: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  text = BeautifulSoup(text, \"html.parser\").get_text()\n",
            "<ipython-input-38-ea37174c58eb>:22: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  text = BeautifulSoup(text, \"html.parser\").get_text()\n",
            "<ipython-input-38-ea37174c58eb>:22: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              news_title  \\\n",
            "0      A dairy-for-lumber deal? Think-tank paper prop...   \n",
            "1      A look at what's driving cooling electric vehi...   \n",
            "2      Air Canada lands last in on-time flights in ra...   \n",
            "3      Alberta's carbon capture rollout plan criticiz...   \n",
            "4      'America First' strategy is raising hackles, a...   \n",
            "...                                                  ...   \n",
            "19992  YOU GOTTA LOVE THIS: [VIDEO] White Girl Told S...   \n",
            "19993  YOU WON?T BELIEVE Why Students In Communist Wi...   \n",
            "19994  Yahoo caves in to NSA, FBI ? and secretly moni...   \n",
            "19995  YALE DISFIGURES STONE CARVING to Disarm Purita...   \n",
            "19996  Trump's $175M Bond Rejected, Filled With Filin...   \n",
            "\n",
            "                                         processed_title  \\\n",
            "0      dairyforlumber deal thinktank paper proposes c...   \n",
            "1      look whats driving cooling electric vehicle sa...   \n",
            "2      air canada lands last ontime flights ranking n...   \n",
            "3      albertas carbon capture rollout plan criticize...   \n",
            "4      america first strategy raising hackles canada ...   \n",
            "...                                                  ...   \n",
            "19992  got ta love video white girl told shes allowed...   \n",
            "19993  wont believe students communist wisconsin long...   \n",
            "19994  yahoo caves nsa fbi secretly monitors customer...   \n",
            "19995  yale disfigures stone carving disarm puritan p...   \n",
            "19996          trumps bond rejected filled filing errors   \n",
            "\n",
            "                                               news_text  \\\n",
            "0      The most common uses of Canadian dairy normall...   \n",
            "1      Several automakers are pulling back onÂ plans t...   \n",
            "2      Air Canada notched the worst on-time performan...   \n",
            "3      Industry leaders in Alberta have criticized th...   \n",
            "4      At first it was easy to discount U. S. Preside...   \n",
            "...                                                  ...   \n",
            "19992  Try telling a young black girl she can t wear ...   \n",
            "19993  This is out of control! Is there anything more...   \n",
            "19994  21st Century Wire says This will rank as one o...   \n",
            "19995   Idiots! That s the first comment at Yale Alum...   \n",
            "19996  Again I say, if you wrote this as a novel, nob...   \n",
            "\n",
            "                                          processed_text  \n",
            "0      common uses canadian dairy normally include mi...  \n",
            "1      several automakers pulling back plans expand n...  \n",
            "2      air canada notched worst ontime performance am...  \n",
            "3      industry leaders alberta criticized government...  \n",
            "4      first easy discount u president donald trumps ...  \n",
            "...                                                  ...  \n",
            "19992  try telling young black girl wear dreadlocks s...  \n",
            "19993  control anything unamerican telling high schoo...  \n",
            "19994  st century wire says rank one egregious miscar...  \n",
            "19995  idiots first comment yale alumni magazine twit...  \n",
            "19996  say wrote novel nobody would believe simply wo...  \n",
            "\n",
            "[19997 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Training:"
      ],
      "metadata": {
        "id": "UwP0VQm-gwVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "\n",
        "#Dataframe loading after preprocessing\n",
        "titles = df['processed_title'].values\n",
        "texts = df['processed_text'].values\n",
        "df['label'] = df['news_label'].map({'Real': 1, 'Fake': 0})\n",
        "labels = df['label'].values\n",
        "\n",
        "\n",
        "#Combining title and text:\n",
        "combined_texts = [title + \" \" + text for title, text in zip(titles, texts)]\n",
        "\n",
        "#Tokenizing the combined text data\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(combined_texts)\n",
        "sequences = tokenizer.texts_to_sequences(combined_texts)\n",
        "\n",
        "#Padding sequences\n",
        "maxlen = 1000\n",
        "padded_sequences = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "#Splitting data into training and testing parts\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "#Building the 1D CNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=5000, output_dim=128, input_length=maxlen))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#Training the model\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "#Testing:\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "au0hz8SBgvVE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8297a92a-5272-4b92-8333-b61cd454aa13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "500/500 [==============================] - 257s 509ms/step - loss: 0.1890 - accuracy: 0.9149 - val_loss: 0.0657 - val_accuracy: 0.9753\n",
            "Epoch 2/5\n",
            "500/500 [==============================] - 258s 516ms/step - loss: 0.0283 - accuracy: 0.9909 - val_loss: 0.0621 - val_accuracy: 0.9800\n",
            "Epoch 3/5\n",
            "500/500 [==============================] - 254s 509ms/step - loss: 0.0035 - accuracy: 0.9995 - val_loss: 0.0735 - val_accuracy: 0.9785\n",
            "Epoch 4/5\n",
            "500/500 [==============================] - 252s 504ms/step - loss: 4.4954e-04 - accuracy: 1.0000 - val_loss: 0.0713 - val_accuracy: 0.9800\n",
            "Epoch 5/5\n",
            "500/500 [==============================] - 249s 499ms/step - loss: 2.3077e-04 - accuracy: 1.0000 - val_loss: 0.0775 - val_accuracy: 0.9812\n",
            "125/125 [==============================] - 15s 121ms/step - loss: 0.0775 - accuracy: 0.9812\n",
            "Test Accuracy: 0.981249988079071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Saving"
      ],
      "metadata": {
        "id": "9YNgBd5AN-uP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model saving .h5 format\n",
        "model.save('/content/drive/MyDrive/FYP/news_detection_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEfBp1EYOA6E",
        "outputId": "499417ea-100a-47e7-a124-4707e7b856fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    }
  ]
}